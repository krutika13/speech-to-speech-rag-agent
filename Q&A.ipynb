{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##How to duplicate object?(Shallow and Deep Copy?"
      ],
      "metadata": {
        "id": "RiicCFfQYgDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvl5EfZUYejr"
      },
      "outputs": [],
      "source": [
        "#Shallow Copy and Deep Copy\n",
        "#Shallow copy:shallow copy creates new object. if the orginal object contain nested objects so the shallow copy object copy reference of nested object.\n",
        "#so if any chnages in nested shallow object it also reflact in original object.\n",
        "\n",
        "import copy\n",
        "\n",
        "original = [[1, 2], [3, 4]]\n",
        "shallow = copy.copy(original)\n",
        "\n",
        "shallow[0][0] = 99\n",
        "print(original)  # [[99, 2], [3, 4]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep Copy: Deep copy creates new object for each nested object as well. so any changes in copy object will non refleact at original object\n",
        "import copy\n",
        "\n",
        "original = [[1, 2], [3, 4]]\n",
        "deep = copy.deepcopy(original)\n",
        "\n",
        "deep[0][0] = 99\n",
        "print(original)  # [[1, 2], [3, 4]]\n"
      ],
      "metadata": {
        "id": "n9GnxX9lYnJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In python decorators is used to add or modify any function functionality by passig a function as argument in decorated function and it return new function.\n",
        "\n",
        "#Can you write a custom decorator that shows times the execution of a function?\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def timer(func):\n",
        "    def wrapper():\n",
        "        start = time.time()\n",
        "        func()\n",
        "        end = time.time()\n",
        "        print(f\"Time taken: {end - start:.2f} seconds\")\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@timer\n",
        "def good_morning():\n",
        "    print(\"Good Morning\")\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "good_morning()\n"
      ],
      "metadata": {
        "id": "S8tKrNJZYpbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does Python's garbage collection (GC) work, and how can it affect performance?\n",
        "Garbage collection is automaically done in python.\n",
        "it is affect the performance mainly while dealing with recurrsive senario.or while programe take pause or memory overhead\n"
      ],
      "metadata": {
        "id": "akPQOyBkYvqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write a Python function that fetches data from two APIs simultaneously using asyncio and await."
      ],
      "metadata": {
        "id": "IYiTPzDgZ6Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# Simulate an API call\n",
        "async def fake_api(name, delay):\n",
        "    print(f\"Calling {name} API...\")\n",
        "    await asyncio.sleep(delay)\n",
        "    print(f\"{name} API response received\")\n",
        "    return f\"Data from {name}\"\n",
        "\n",
        "# Main async function\n",
        "async def fetch_from_two_apis():\n",
        "    task1 = fake_api(\"API 1\", 2)\n",
        "    task2 = fake_api(\"API 2\", 3)\n",
        "    result1, result2 =  await asyncio.gather(task1, task2)\n",
        "    print(\"\\nResults:\")\n",
        "    print(result1)\n",
        "    print(result2)\n",
        "\n",
        "\n",
        "await fetch_from_two_apis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32U5pp2LZ44i",
        "outputId": "0c0a851c-e364-432f-87aa-224517978d39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling API 1 API...\n",
            "Calling API 2 API...\n",
            "API 1 API response received\n",
            "API 2 API response received\n",
            "\n",
            "Results:\n",
            "Data from API 1\n",
            "Data from API 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is the GIL (Global Interpreter Lock) in Python, and how does it affect concurrency with async programming?\n",
        "\n",
        "GIL is mutext in python. that means a single code bloecck can access by a single thread at a time which helps in memery management.\n",
        "\n",
        "**Impact on Concurrency:**\n",
        "\n",
        "Enables efficient single-threaded concurrency using coroutines and an event loop. Ideal for I/O-bound tasks and avoids GIL limitations for async task\n"
      ],
      "metadata": {
        "id": "MMGTA9bvfYvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How do you persist data in Docker containers? Explain the use of volumes and bind mounts.\n",
        "to persist data from Docker container there is 2 methods, Volums and bind mount\n",
        "\n",
        "--> Bind mount maps local directory with the container directory and what ever changes you made in local directory it will reflact on the container dicrectory.\n",
        "\n",
        "-->Volumes in docker that persist directory by creating new in docker itself.Volumes create directory in var/lib/docker/volumes.\n"
      ],
      "metadata": {
        "id": "inV7DWwVpfEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Given an unsorted array of integers, you need to write a program to find the length of the longest consecutive elements sequence.\n",
        "For example,\n",
        "Input: [100, 4, 200, 1, 3, 2]\n",
        "Output: 4 # The sequence [1, 2, 3, 4] is the longest consecutive sequence."
      ],
      "metadata": {
        "id": "NRL4qQF5uXmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr=[100,4, 5,4, 200, 1, 3, 2]"
      ],
      "metadata": {
        "id": "X6yVKwpupeuT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcrjxIZSwrp9",
        "outputId": "0d046587-039a-49a1-d7eb-012f42768809"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1, 2, 3, 4, 5, 100, 200}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr.sort()"
      ],
      "metadata": {
        "id": "OGHG3tPrfRh1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJZ1fve8vDm0",
        "outputId": "f5162337-d4fe-441a-d0fd-7d010deb85fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 100, 200]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "j=1\n",
        "\n",
        "for k in range(0,len(arr)):\n",
        "  if arr[j]-arr[i]==1:\n",
        "    i+=1\n",
        "    j+=1\n",
        "  else:\n",
        "    print(f\"the lengthe of longest consecutive sequence is {j} \")\n",
        "    print(f\"the sequence is {arr[0:j]}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DocMt2xyZYz4",
        "outputId": "1b1cd17a-9479-41be-8353-199e6f532844"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the lengthe of longest consecutive sequence is 5 \n",
            "the sequence is [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def longest_consecutive(nums):\n",
        "    num_set = set(nums)\n",
        "    longest = 0\n",
        "\n",
        "    for n in num_set:\n",
        "\n",
        "        if n - 1 not in num_set:\n",
        "            length = 1\n",
        "            while n + length in num_set:\n",
        "                length += 1\n",
        "            longest = max(longest, length)\n",
        "\n",
        "    return longest\n",
        "\n",
        "# Example usage\n",
        "print(longest_consecutive([100, 4, 200, 1, 3, 2]))  # Output: 4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVuHLVFkuy_g",
        "outputId": "1409fb20-024c-4266-bc1e-043f9ab42d94"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Can you provide an example of asynchronous route handling in FastAPI?"
      ],
      "metadata": {
        "id": "YGSiuY5AX9N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import asyncio\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/ping\")\n",
        "async def ping():\n",
        "    await asyncio.sleep(1)  # Simulate a delay (e.g., I/O, DB call)\n",
        "    return {\"message\": \"pong\"}\n"
      ],
      "metadata": {
        "id": "twSfRGufxFLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Why is FastAPI generally considered faster than Flask?\n",
        "FastAPI is faster than Flask mainly because it supports asynchronous programming natively through ASGI, async and await.\n",
        "\n",
        " Flask is synchronous and WSGI-based. FastAPI also benefits from Pydantic for fast data validation and also support Uvicorn as server"
      ],
      "metadata": {
        "id": "zU4UvtKRYReq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How would you handle missing data in a Pandas DataFrame? Describe at least three strategies.\n",
        "\n",
        "If the missing data is more than 80 % of total colum data then drop the colum or feature.\n",
        "\n",
        "Other Techniques to handle missing data:\n",
        "\n",
        "1) Replace missing values with 'Unknown' or '0'\n",
        "\n",
        "2)Apply imputation technique like Mode(means or median) imputation, replace missingreplace value with most frequent data point.\n",
        "\n",
        "3)replace missing value by forward/backward fill for time series"
      ],
      "metadata": {
        "id": "hFwr5FrjY8Ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How would you optimize a large Pandas DataFrame for performance and memory usage?\n",
        "\n",
        "To optimize large Pandas DataFrames, I first reduce memory usage by using proper data types (like int32 or category), drop unused columns, and use vectorized operations. I also read large files in chunks and use memory-efficient methods like query() or eval() for faster computation. This helps ensure scalability and responsiveness in data pipelines."
      ],
      "metadata": {
        "id": "zbiXDnBJaJlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Can you explain broadcasting in Numpy and provide a practical example?\n",
        "\n",
        "With help of broadcasting NumPy can perform operations on arrays of different shapes by automatically expanding the smaller array so their shapes match."
      ],
      "metadata": {
        "id": "Cf1-PmdnbMLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "\n",
        "b = np.array([10, 20, 30])\n",
        "\n",
        "result = a + b\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "xdSRAAXMY7-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the difference between an INNER JOIN and a LEFT JOIN in SQL?\n",
        "\n",
        "In inner join the resultant table contain both table comman colums. means it will return table with only matched colums.\n",
        "\n",
        "Left Joins returns all records from the left table and fills in NULLs for non-matching records from the right."
      ],
      "metadata": {
        "id": "fKfP5FjPbjsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is normalization? Explain 3NF\n",
        "Normalization is the process of structuring a relational database to eliminate redundancy and improve integrity. Third Normal Form (3NF) ensures that every non-key column is only dependent on the primary key not on other non-key columns. This removes transitive dependencies and improves data consistency"
      ],
      "metadata": {
        "id": "MF7RD0zrcK5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is database sharding?\n",
        "Database sharding is a horizontal partitioning technique that splits data across multiple databases or servers. It improves scalability and performance by distributing the load, especially for large-scale systems. However, it introduces challenges like cross-shard joins, rebalancing, and consistency management."
      ],
      "metadata": {
        "id": "UBRPUMm0cnhn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yhpKZeNYRI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}